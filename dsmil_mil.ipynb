{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import mil_dsmil_softmax as mil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channel, output_class=1):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.fc = nn.Linear(in_channel, output_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = self.fc(x)\n",
    "        return x, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df[df.columns[0]]\n",
    "    data_list = []    \n",
    "    for i in range(0, df.shape[0]):  \n",
    "        data = str(df.iloc[i]).split(' ')\n",
    "        ids = data[0].split(':')\n",
    "        idi = int(ids[0])\n",
    "        idb = int(ids[1])\n",
    "        idc = int(ids[2])\n",
    "        data = data[1:]\n",
    "        feature_vector = np.zeros(len(data))  \n",
    "        for i, feature in enumerate(data):\n",
    "            feature_data = feature.split(':')\n",
    "            if len(feature_data) == 2:\n",
    "                feature_vector[i] = feature_data[1]\n",
    "        data_list.append([idi, idb, idc, feature_vector])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag(data, idb):\n",
    "    data_array = np.array(data)\n",
    "    bag_id = data_array[:, 1]\n",
    "    return data_array[np.where(bag_id == idb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(bag_ins_list):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(bag_ins_list):\n",
    "        optimizer.zero_grad()\n",
    "        data_bag_list = shuffle(data[1])\n",
    "        data_tensor = torch.from_numpy(np.stack(data_bag_list)).float().cuda()\n",
    "        data_tensor = data_tensor[:, 0:num_feats]\n",
    "        label = torch.from_numpy(np.array(int(np.clip(data[0], 0, 1)))).float().cuda()\n",
    "#         print(label)\n",
    "        classes, bag_prediction, _ = milnet(data_tensor) # n X L\n",
    "#         print(A, 1)\n",
    "        max_prediction, index = torch.max(classes, 0)\n",
    "        loss_bag = mse_loss(bag_prediction.view(1, -1), label.view(1, -1))\n",
    "        loss_max = mse_loss(max_prediction.view(1, -1), label.view(1, -1))\n",
    "        loss_total = 0.75*loss_bag + 0.25*loss_max\n",
    "        loss_total = loss_total.mean()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()  \n",
    "        epoch_loss = epoch_loss + loss_total.item()\n",
    "    return epoch_loss / len(bag_ins_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_test(bag_ins_list):\n",
    "    bag_labels = []\n",
    "    bag_predictions = []\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(bag_ins_list):\n",
    "            bag_labels.append(np.clip(data[0], 0, 1))\n",
    "            data_tensor = torch.from_numpy(np.stack(data[1])).float().cuda()\n",
    "            data_tensor = data_tensor[:, 0:num_feats]\n",
    "            label = torch.from_numpy(np.array(int(np.clip(data[0], 0, 1)))).float().cuda()\n",
    "            classes, bag_prediction, _ = milnet(data_tensor) # n X L\n",
    "            max_prediction, index = torch.max(classes, 0)\n",
    "            loss_bag = mse_loss(bag_prediction.view(1, -1), label.view(1, -1))\n",
    "            loss_max = mse_loss(max_prediction.view(1, -1), label.view(1, -1))\n",
    "            loss_total = 0.75*loss_bag + 0.25*loss_max\n",
    "            loss_total = loss_total.mean()\n",
    "#             bag_predictions.append(0.75*bag_prediction.cpu().squeeze().numpy() + 0.25*max_prediction.cpu().squeeze().numpy())\n",
    "            bag_predictions.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())\n",
    "            epoch_loss = epoch_loss + loss_total.item()\n",
    "    epoch_loss = epoch_loss / len(bag_ins_list)\n",
    "    return epoch_loss, bag_labels, bag_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_scores(bag_labels, bag_predictions):\n",
    "    fpr, tpr, threshold = roc_curve(bag_labels, bag_predictions, pos_label=1)\n",
    "    fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "    auc_value = roc_auc_score(bag_labels, bag_predictions)\n",
    "    this_class_label = np.array(bag_predictions)\n",
    "    this_class_label[this_class_label>=threshold_optimal] = 1\n",
    "    this_class_label[this_class_label<threshold_optimal] = 0\n",
    "    bag_predictions = this_class_label\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(bag_labels, bag_predictions, average='binary')\n",
    "    accuracy = 1- np.count_nonzero(np.array(bag_labels).astype(int)- bag_predictions.astype(int)) / len(bag_labels)\n",
    "#     print(bag_labels)\n",
    "#     print(bag_predictions)\n",
    "    return accuracy, auc_value, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_set(in_list, fold, index):\n",
    "    csv_list = copy.deepcopy(in_list)\n",
    "    n = int(len(csv_list)/fold)\n",
    "    chunked = [csv_list[i:i+n] for i in range(0, len(csv_list), n)]\n",
    "    test_list = chunked.pop(index)\n",
    "    return list(itertools.chain.from_iterable(chunked)), test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_pos_weight(bags_list):\n",
    "    pos_count = 0\n",
    "    for item in bags_list:\n",
    "        pos_count = pos_count + np.clip(item[0], 0, 1)\n",
    "    return (len(bags_list)-pos_count)/pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musk 1\n",
    "data_all = get_data('mil_dataset/Musk/musk2norm.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_ins_list = []\n",
    "num_bag = data_all[-1][1]+1\n",
    "# print(num_bag)\n",
    "for i in range(num_bag):\n",
    "    bag_data = get_bag(data_all, i)\n",
    "    bag_label = bag_data[0, 2]\n",
    "    bag_vector = bag_data[:, 3]\n",
    "    print('bag')\n",
    "    print(bag_data.shape)\n",
    "    bag_ins_list.append([bag_label, bag_vector])\n",
    "bag_ins_list = shuffle(bag_ins_list)\n",
    "# print(bag_ins_list)\n",
    "print(len(bag_ins_list))\n",
    "bags_list, test_list = cross_validation_set(bag_ins_list, fold=10, index=1)\n",
    "# print(len(test_list))\n",
    "print(len(bags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(compute_pos_weight(bags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acs = []\n",
    "num_feats = 166\n",
    "for k in range(0, 10):\n",
    "    bags_list, test_list = cross_validation_set(bag_ins_list, fold=10, index=k)\n",
    "    i_classifier = FeatureExtractor(num_feats, 1)\n",
    "    b_classifier = mil.BClassifier(input_size=num_feats, output_class=1)\n",
    "    milnet = mil.MILNet(i_classifier, b_classifier).cuda()\n",
    "#     mse_loss = nn.MSELoss()\n",
    "    pos_weight = torch.tensor(compute_pos_weight(bags_list))\n",
    "    mse_loss = nn.BCEWithLogitsLoss(pos_weight)\n",
    "    optimizer = torch.optim.Adam(b_classifier.parameters(), lr=0.0001, betas=(0.5, 0.9), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, 0.1)\n",
    "    optimal_ac = 0\n",
    "    for epoch in range(0, 100):\n",
    "        train_loss = epoch_train(bags_list) # iterate all bags\n",
    "#         print('\\r[%d/%d] train loss: %.4f' % (epoch, 100, train_loss))\n",
    "        test_loss, bag_labels, bag_predictions = epoch_test(test_list)\n",
    "        accuracy, auc_value, precision, recall, fscore = five_scores(bag_labels, bag_predictions)\n",
    "#         print('\\r        test loss: %.4f, average score: %.4f, aug score: %.4f, precision: %.4f, recall: %.4f, fscore: %.4f' % \n",
    "#               (test_loss, accuracy, auc_value, precision, recall, fscore))\n",
    "\n",
    "        optimal_ac = max(accuracy, optimal_ac)\n",
    "\n",
    "        scheduler.step()\n",
    "    print('Optimal ac: %.4f' % \n",
    "              (optimal_ac))\n",
    "    acs.append(optimal_ac)\n",
    "print('mean: %.4f, std %.4f' % (np.mean(np.array(acs)), np.std(np.array(acs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.from_numpy(np.array([[1, 5, 4, 6], [2, 2, 4, 4]]))\n",
    "B = torch.from_numpy(np.array([1, 2, 3, 4]))\n",
    "# _, indices = torch.sort(A, 0, True) \n",
    "# B = torch.index_select(B, 0, indices)\n",
    "print(A * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
